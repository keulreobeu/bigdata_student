# word 2 vec

기존의 자연어 처리는 원핫 인코딩처럼 단어를 원자취급하여 단어간 유사성을 표현하지 못한 체 사용하였다.

대표적으로 통계적 언어 모델링에서 사용된 N-gram처럼 어떤단어가 어떻게 많이 사용되었다 정도만 알 수 있었다.

이는 원 핫 인코딩처럼 0과 1로만 표현하는 희소표현으로 인해 단어간 유사성을 표현하지 못하는 점에서 시작되었다.

word 2 vec는 이러한 문제를 해결하기 위해 다차원 공간에서 분산표현을 통해 문자의 백터를 학습 생성하여 코사인 유사도와 같이 유사성을 가질 수 있는 백터를 통해 단어의 정보를 저장하여 위의 문제를 해결하려 하였다.

word 2 vec에는 2가지 방법을 통해 학습하여 백터를 생성한다.

### CBOW(countinuous Bag of Words)
CBOW방식은 타겟 단어 주변의 단어를 이용하여 타겟 단어를 예측하는 방식입니다.
윈도우 크기가 2라면 타겟 단어 앞 뒤 2개씩 총 4개의 단어를 이용하여 가운데 단어를 예측하여 백터를 만듭니다.

### Skip-fram
CBOW와 반대로 중심단어를 통해 주변 단어를 예측하는 방식입니다.
윈도우가 2라면 중심단어 앞 뒤 2단어씩 4단어가 어떤 단어가 올 지 예측하여 백터를 만듭니다.




# Transformer
Transformer는 attention만 사용하여 seq2seq구로조 만든 인코더-디코더 모델입니다.
RNN모델을 사용한 것 보다 더 우수한 성능을 보여준다.

기존의 seq2seq모델의 한계는 입력시퀸스를 하나의 백터로 압축표현을 하고 디코더로 출력을 하고 이때 입력에서 일부 소실이 일어나서 이 소실을 보정하기 위해 매 시점마다 전체 입력 문장을 다시 참고하고 더 나아가 예측해야할 단어와 연관이 있는 부분을 더 집중하게 해주는 attention을 사용했다.

하지만 transformer는 이 attention만을 이용하여 모델을 만들어 사용하여 입력 손실을 막고 여러개의 인코더-디코더를 통해 여러 시점을 분석하고 연관성 있는 부분을 잘 학습할수 있게 되었습니다.

### Transformer의 구조
![image](https://github.com/keulreobeu/bigdate_student/assets/112425846/7e5d14b7-0034-4f69-b9f6-f7feae39bdb4)
인코더에 들어오기 전 임베딩을 통해 문장을 백터화 하여 단어로 쪼갠 뒤 각 단어들의 순서정보를 가지게 하는데 이를 포지셔널 인코딩을 진행 후 인코더에 진입한다.
그 후 Multi-head self-Attention을 진행하는데 각각의 Quert에 대에서 모든 Key의 유사도를 구하여 각 키에 맵핑되어 있는 Value에 반영해줍니다. 이러한 밸류를 가중치가 다른 여러개의 self-Attention을 통해 계산하고 이 결과의 가중합을 반환한다. 이 가중합을 통해 단어가 어떤 의미로 사용되었는지 유추가 가능하다. 
그 후 Position-wise Feed Forward Neural Network에 Attention의 값이 전달되고 입력된 동일한 크기의 행렬로 연산되어 반환됩니다.

마지막으로 각 Attention이나 FFNN을 진행후 add(잔차연결)과 Norm(정규화)를 진행합니다.

잔차연결은 각 행렬에 잔차를 덧셈연산을 하고 정규화를 통해 기울기를 조절하여 기울기 소실이나 기울기 폭주를 완화시키는 역할을 한다. 이는 반복적인 자기회귀 연산을 하는 트렌스포머에서 기울기소실이나 폭주를 막아주는 역할을 한다.

디코더도 인코더에서 처럼 입력을 단어를 임베딩하여 문장행렬을 입력받는다. 이때 들어가는 문장은 정답을 가지는 문장으로 정답이 없다면 학습하지 못한다. 그리고 인코더와 다르게 Masked Multi-head self-Attention을 통해 처리되는데 이는 문장의 일부를 가려 현재 시점보다 미래시점의 단어를 보지 못하게 하여 현재시점의 값만 다음 서브층으로 넘겨 줄 수 있도록 한다.

인코더에서 나온 출력과 디코더 첫번째 서브층에서 나오는 출력이 두번째 서브층으로가게 되는데 비슷한 Multi-head Attention이지만 서로다른곳에서 온 값을 이용함으로 self가 아니다.  self-Attention은 Key, Value, Query모두 자기의 것을 사용하지만 디코더의 Attention의 Query는 첫번째 서브층에서 나온 Query를 사용하여 계산한다.

세번째 서브층인 FFNN을 통과한 후 나온 out put을 이용하여 정답을 만들고 자기회귀학습을 통해 학습을 진행한다.



# BERT 
BERT는 문맥을 이해하여 학습하는 것을 목표로 만들어진 언어모델로 기존의 언어 학습방식처럼 왼쪽에서 오른쪽으로 학습하는 것이 아닌 양방향으로 문장을 학습하는 모델을 말한다.

구조는 트렌스포머의 인코더를 쌓아올린 구조로 토큰이 들어왔을 때 일정비율울 뮤작위 마스킹을 한 후 마스킹된 토큰을 예측하는 방식으로 학습을 진행한다.




#GPT
트랜스포머의 디코더를 활용한 모델로 특정 값을 주었을때 다음 내용을 생성하도록 만든 모델
